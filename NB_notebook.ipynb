{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "compressed-tractor",
   "metadata": {},
   "source": [
    "# Importing needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "based-atlanta",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-spotlight",
   "metadata": {},
   "source": [
    "# Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "under-cooperation",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "rows = []\n",
    "columns = ['statement', 'fileName', 'class']\n",
    "for roots, dirs, files in os.walk(path+ '/bestfriend.deception.training'):\n",
    "    for file in files:\n",
    "        #rows.append()\n",
    "        if file.split()[0].startswith('lie'):\n",
    "            doc = open(roots+'/'+file, 'r')\n",
    "            doc_str = doc.read()\n",
    "            rows.append([doc_str, file, 'lie'])\n",
    "            doc.close()\n",
    "        elif file.split()[0].startswith('true'):\n",
    "            doc = open(roots+'/'+file, 'r')\n",
    "            doc_str = doc.read()\n",
    "            rows.append([doc_str, file,'true'])\n",
    "            doc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "binding-craft",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             statement    fileName class\n",
      "0    He's a really nice person. He is always willin...    lie7.txt   lie\n",
      "1    \"This person is quite accomplished.  She has d...   lie12.txt   lie\n",
      "2    My best friend is always there when I need her...  true60.txt  true\n",
      "3    My best friend is an amazing person. He is nev...    lie8.txt   lie\n",
      "4    This person is my best friend. Was always ther...   lie11.txt   lie\n",
      "..                                                 ...         ...   ...\n",
      "191  Cassandra is a great friend. We have grown los...   lie58.txt   lie\n",
      "192  I've known my best friend for 38 years. We gre...  true76.txt  true\n",
      "193  Jean is my best friend.  I have known her for ...  true83.txt  true\n",
      "194  I met my best friend when I was a freshman on ...   lie72.txt   lie\n",
      "195  \"My best friend is very funny. He's always mak...   lie20.txt   lie\n",
      "\n",
      "[196 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "training_data = pd.DataFrame(rows, columns = columns)\n",
    "print(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "first-sleeve",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2071\n"
     ]
    }
   ],
   "source": [
    "def trainNaiveBayes(train_data, stop_word_rm = False, stem_rm = False):\n",
    "    \n",
    "    \n",
    "    word_in_class_count = {}\n",
    "    all_true = []\n",
    "    all_lie = []\n",
    "    word_in_class_probability = {}\n",
    "    \n",
    "    # remove punctuation, tokenize\n",
    "    tokenized_list = []\n",
    "    exclude = set(string.punctuation)\n",
    "    for index, row in train_data.iterrows():\n",
    "        punct_removed = ''.join(ch for ch in row['statement'] if ch not in exclude)\n",
    "        tokenList = nltk.word_tokenize(punct_removed)\n",
    "        tokenList = [word.lower() for word in tokenList]\n",
    "        if stem_rm:\n",
    "            p_stemmer = PorterStemmer()\n",
    "            tokenList = [p_stemmer.stem(word) for word in tokenList]\n",
    "            \n",
    "        if stop_word_rm:\n",
    "            nltk_stop_words = set(stopwords.words(\"english\"))\n",
    "            tokenList = [w for w in tokenList if w not in nltk_stop_words]\n",
    "        tokenized_list.append(tokenList)\n",
    "    if 'tokenizedList' not in train_data:\n",
    "        train_data.insert(1, \"tokenizedList\", tokenized_list, False)\n",
    "        \n",
    "    # creat a dictionary in which the keys are our vocabulary and values are the count of the words in each \n",
    "    # class\n",
    "    for index, row in train_data.iterrows():\n",
    "        for word in row['tokenizedList']:\n",
    "            if word not in word_in_class_count:\n",
    "                if row['class'] == 'lie':\n",
    "                    word_in_class_count[word] = {'lie': 1, 'true': 0}\n",
    "                elif row['class'] == 'true':\n",
    "                    word_in_class_count[word] = {'lie': 0, 'true': 1}\n",
    "            else:\n",
    "                if row['class'] == 'lie':\n",
    "                    word_in_class_count[word]['lie'] += 1\n",
    "                elif row['class'] == 'true':\n",
    "                    word_in_class_count[word]['true'] += 1\n",
    "    # number of words in class true\n",
    "    for index, row in train_data.iterrows():\n",
    "        for word in row['tokenizedList']:\n",
    "            if row['class'] == 'lie':\n",
    "                if word not in all_lie:\n",
    "                    all_lie.append(word)\n",
    "            elif row['class'] == 'true':\n",
    "                if word not in all_true:\n",
    "                    all_true.append(word)\n",
    "    \n",
    "    # word in class probability, add-1 smoothing\n",
    "    V = len(word_in_class_count)\n",
    "    for word in word_in_class_count:\n",
    "        word_in_class_probability[word] = {'lie': (word_in_class_count[word]['lie'] + 1)/(len(all_lie)+V), \n",
    "                                          'true': (word_in_class_count[word]['true'] + 1)/(len(all_true)+V)}\n",
    "    \n",
    "    # probability of each class\n",
    "    lie_count = 0\n",
    "    true_count = 0\n",
    "    for index, row in train_data.iterrows():\n",
    "        if row['class'] == 'lie':\n",
    "            lie_count+=1\n",
    "        elif row['class'] == 'true':\n",
    "            true_count+=1\n",
    "    lie_prob = lie_count / (lie_count + true_count)\n",
    "    true_prob = true_count / (lie_count + true_count)\n",
    "    \n",
    "    return word_in_class_probability, lie_prob, true_prob\n",
    "    \n",
    "            \n",
    "print(len(trainNaiveBayes(training_data.copy())[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "antique-repair",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testNaiveBayes(nbClassifier, test_statement, stop_word_rm = False, stem_rm = False):\n",
    "    exclude = set(string.punctuation)\n",
    "    punct_removed = ''.join(ch for ch in test_statement if ch not in exclude)\n",
    "    tokenList = nltk.word_tokenize(punct_removed)\n",
    "    tokenList = [word.lower() for word in tokenList]\n",
    "    if stem_rm:\n",
    "        p_stemmer = PorterStemmer()\n",
    "        tokenList = [p_stemmer.stem(word) for word in tokenList]\n",
    "\n",
    "    if stop_word_rm:\n",
    "        nltk_stop_words = set(stopwords.words(\"english\"))\n",
    "        tokenList = [w for w in tokenList if w not in nltk_stop_words]\n",
    "        \n",
    "    # probability for lie class\n",
    "    p_lie = 1\n",
    "    for word in tokenList:\n",
    "        p_lie *= nbClassifier[word]['lie']\n",
    "    \n",
    "    p_true = 1\n",
    "    for word in tokenList:\n",
    "        p_true *= nbClassifier[word]['true']\n",
    "    \n",
    "    if p_lie > p_true:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "absolute-tuner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('lie7.txt', False), ('lie12.txt', False), ('true60.txt', True), ('lie8.txt', False), ('lie11.txt', False), ('true20.txt', True), ('true31.txt', True), ('lie28.txt', False), ('lie15.txt', False), ('lie42.txt', False), ('lie26.txt', False), ('true72.txt', True), ('true33.txt', True), ('true56.txt', True), ('lie81.txt', True), ('lie23.txt', False), ('lie71.txt', True), ('true9.txt', True), ('lie24.txt', False), ('true24.txt', True), ('true85.txt', True), ('lie69.txt', False), ('lie45.txt', False), ('lie95.txt', False), ('lie91.txt', True), ('lie62.txt', True), ('true43.txt', True), ('true10.txt', True), ('true67.txt', True), ('true37.txt', True), ('lie27.txt', False), ('true14.txt', True), ('true52.txt', True), ('true66.txt', True), ('true51.txt', True), ('true79.txt', True), ('true70.txt', True), ('true21.txt', True), ('lie33.txt', False), ('lie89.txt', False), ('true11.txt', True), ('lie48.txt', False), ('lie39.txt', False), ('lie73.txt', False), ('true15.txt', True), ('true74.txt', True), ('true12.txt', True), ('true26.txt', True), ('true50.txt', True), ('true87.txt', True), ('true48.txt', True), ('true65.txt', True), ('true3.txt', True), ('lie29.txt', False), ('lie78.txt', False), ('true30.txt', True), ('true41.txt', True), ('lie59.txt', False), ('lie46.txt', True), ('lie80.txt', True), ('lie70.txt', True), ('lie31.txt', True), ('lie96.txt', True), ('lie13.txt', True), ('lie55.txt', False), ('true57.txt', True), ('lie32.txt', True), ('lie63.txt', False), ('true91.txt', True), ('true86.txt', True), ('true89.txt', True), ('lie94.txt', False), ('true62.txt', True), ('lie22.txt', False), ('true39.txt', True), ('lie47.txt', True), ('lie57.txt', True), ('true1.txt', True), ('lie35.txt', False), ('true55.txt', True), ('true4.txt', True), ('lie44.txt', False), ('true35.txt', True), ('true16.txt', True), ('lie18.txt', False), ('true36.txt', True), ('lie61.txt', True), ('lie97.txt', False), ('lie38.txt', True), ('lie21.txt', False), ('true29.txt', True), ('lie6.txt', False), ('lie36.txt', False), ('lie84.txt', False), ('true7.txt', True), ('lie64.txt', False), ('lie90.txt', False), ('true82.txt', True), ('lie37.txt', False), ('true64.txt', True), ('lie14.txt', False), ('true68.txt', True), ('lie16.txt', False), ('true23.txt', True), ('lie1.txt', True), ('true34.txt', True), ('true17.txt', True), ('lie68.txt', False), ('true94.txt', True), ('lie50.txt', False), ('true92.txt', True), ('lie65.txt', False), ('lie40.txt', True), ('lie17.txt', True), ('lie85.txt', False), ('lie51.txt', True), ('lie25.txt', True), ('lie66.txt', False), ('true38.txt', True), ('lie82.txt', True), ('lie19.txt', True), ('lie41.txt', False), ('lie4.txt', True), ('true93.txt', True), ('true6.txt', True), ('lie10.txt', False), ('true69.txt', True), ('true90.txt', True), ('true63.txt', True), ('true40.txt', True), ('true96.txt', True), ('lie2.txt', True), ('lie43.txt', False), ('true78.txt', True), ('lie67.txt', False), ('lie77.txt', True), ('true77.txt', True), ('true49.txt', True), ('true53.txt', True), ('lie87.txt', False), ('true18.txt', True), ('true95.txt', True), ('true88.txt', True), ('true97.txt', True), ('true73.txt', True), ('true98.txt', True), ('true58.txt', True), ('true13.txt', True), ('lie54.txt', False), ('true42.txt', True), ('true71.txt', True), ('true27.txt', True), ('lie88.txt', False), ('lie49.txt', False), ('true59.txt', True), ('lie74.txt', False), ('lie86.txt', False), ('lie76.txt', True), ('true46.txt', True), ('true2.txt', True), ('true25.txt', True), ('lie83.txt', True), ('true32.txt', True), ('true80.txt', True), ('lie52.txt', True), ('lie3.txt', False), ('lie53.txt', False), ('true28.txt', True), ('lie93.txt', False), ('true22.txt', True), ('lie98.txt', False), ('true84.txt', True), ('true19.txt', True), ('lie92.txt', False), ('lie75.txt', True), ('true61.txt', True), ('lie60.txt', True), ('true81.txt', True), ('true45.txt', True), ('true47.txt', True), ('lie79.txt', True), ('lie9.txt', False), ('lie34.txt', True), ('lie56.txt', True), ('lie30.txt', False), ('true54.txt', True), ('true5.txt', True), ('true8.txt', True), ('lie5.txt', False), ('true44.txt', True), ('true75.txt', True), ('lie58.txt', True), ('true76.txt', True), ('true83.txt', True), ('lie72.txt', True), ('lie20.txt', False)]\n",
      "accuracy: 0.8214285714285714\n"
     ]
    }
   ],
   "source": [
    "#len(training_data)\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "fileName_class = []\n",
    "\n",
    "for i in range(len(training_data)):\n",
    "    train = training_data.iloc[0:i]\n",
    "    train2 = training_data.iloc[i+1:len(training_data)]\n",
    "    train = pd.concat([train,train2])\n",
    "    test = training_data.iloc[i]['statement']\n",
    "    check = training_data.iloc[i]['class']\n",
    "    fname = training_data.iloc[i]['fileName']\n",
    "    classifier = trainNaiveBayes(training_data.copy())[0]\n",
    "    pred_class = testNaiveBayes(classifier, test)\n",
    "    fileName_class.append((fname, pred_class))\n",
    "    if pred_class == True and check == 'true':\n",
    "        tp += 1\n",
    "    elif pred_class == True and check == 'lie':\n",
    "        fp += 1\n",
    "    elif pred_class == False and check == 'lie':\n",
    "        tn += 1\n",
    "    elif pred_class == False and check == 'true':\n",
    "        fn += 1\n",
    "\n",
    "answers = open(os.getcwd() + '/answers.txt', 'w')\n",
    "print(fileName_class)\n",
    "print(f'accuracy: {(tp + tn)/ (tp+fp+fn+tn)}')\n",
    "answers.write(f'accuracy (stopword_removal = false, stemming = false): {(tp + tn)/ (tp+fp+fn+tn)}\\n')\n",
    "answers.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-yugoslavia",
   "metadata": {},
   "source": [
    "# Write-up guidelines:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-database",
   "metadata": {},
   "source": [
    "## Accuracy when stopwords are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "beginning-organization",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9591836734693877\n"
     ]
    }
   ],
   "source": [
    "#len(training_data)\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "fileName_class = []\n",
    "\n",
    "for i in range(len(training_data)):\n",
    "    train = training_data.iloc[0:i]\n",
    "    train2 = training_data.iloc[i+1:len(training_data)]\n",
    "    train = pd.concat([train,train2])\n",
    "    test = training_data.iloc[i]['statement']\n",
    "    check = training_data.iloc[i]['class']\n",
    "    fname = training_data.iloc[i]['fileName']\n",
    "    classifier = trainNaiveBayes(training_data.copy(), True, False)[0]\n",
    "    pred_class = testNaiveBayes(classifier, test, True, False)\n",
    "    fileName_class.append((fname, pred_class))\n",
    "    if pred_class == True and check == 'true':\n",
    "        tp += 1\n",
    "    elif pred_class == True and check == 'lie':\n",
    "        fp += 1\n",
    "    elif pred_class == False and check == 'lie':\n",
    "        tn += 1\n",
    "    elif pred_class == False and check == 'true':\n",
    "        fn += 1\n",
    "\n",
    "answers = open(os.getcwd() + '/answers.txt', 'a')\n",
    "print(f'accuracy: {(tp + tn)/ (tp+fp+fn+tn)}')\n",
    "answers.write(f'accuracy (stopword_removal = True, stemming = false): {(tp + tn)/ (tp+fp+fn+tn)}\\n')\n",
    "answers.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepting-traffic",
   "metadata": {},
   "source": [
    "## Accuracy when stemming is applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "widespread-smell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8061224489795918\n"
     ]
    }
   ],
   "source": [
    "#len(training_data)\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "fileName_class = []\n",
    "\n",
    "for i in range(len(training_data)):\n",
    "    train = training_data.iloc[0:i]\n",
    "    train2 = training_data.iloc[i+1:len(training_data)]\n",
    "    train = pd.concat([train,train2])\n",
    "    test = training_data.iloc[i]['statement']\n",
    "    check = training_data.iloc[i]['class']\n",
    "    fname = training_data.iloc[i]['fileName']\n",
    "    classifier = trainNaiveBayes(training_data.copy(), False, True)[0]\n",
    "    pred_class = testNaiveBayes(classifier, test, False, True)\n",
    "    fileName_class.append((fname, pred_class))\n",
    "    if pred_class == True and check == 'true':\n",
    "        tp += 1\n",
    "    elif pred_class == True and check == 'lie':\n",
    "        fp += 1\n",
    "    elif pred_class == False and check == 'lie':\n",
    "        tn += 1\n",
    "    elif pred_class == False and check == 'true':\n",
    "        fn += 1\n",
    "\n",
    "answers = open(os.getcwd() + '/answers.txt', 'a')\n",
    "print(f'accuracy: {(tp + tn)/ (tp+fp+fn+tn)}')\n",
    "answers.write(f'accuracy (stopword_removal = false, stemming = true): {(tp + tn)/ (tp+fp+fn+tn)}\\n')\n",
    "answers.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-injury",
   "metadata": {},
   "source": [
    "## Accuracy when stopwords are removed and stemming is applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "chubby-treasury",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9489795918367347\n"
     ]
    }
   ],
   "source": [
    "#len(training_data)\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "fileName_class = []\n",
    "\n",
    "for i in range(len(training_data)):\n",
    "    train = training_data.iloc[0:i]\n",
    "    train2 = training_data.iloc[i+1:len(training_data)]\n",
    "    train = pd.concat([train,train2])\n",
    "    test = training_data.iloc[i]['statement']\n",
    "    check = training_data.iloc[i]['class']\n",
    "    fname = training_data.iloc[i]['fileName']\n",
    "    classifier = trainNaiveBayes(training_data.copy(), True, True)[0]\n",
    "    pred_class = testNaiveBayes(classifier, test, True, True)\n",
    "    fileName_class.append((fname, pred_class))\n",
    "    if pred_class == True and check == 'true':\n",
    "        tp += 1\n",
    "    elif pred_class == True and check == 'lie':\n",
    "        fp += 1\n",
    "    elif pred_class == False and check == 'lie':\n",
    "        tn += 1\n",
    "    elif pred_class == False and check == 'true':\n",
    "        fn += 1\n",
    "\n",
    "answers = open(os.getcwd() + '/answers.txt', 'a')\n",
    "print(f'accuracy: {(tp + tn)/ (tp+fp+fn+tn)}')\n",
    "answers.write(f'accuracy (stopword_removal = true, stemming = true): {(tp + tn)/ (tp+fp+fn+tn)}\\n')\n",
    "answers.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-fifty",
   "metadata": {},
   "source": [
    "# Sorting probabilities\n",
    "Using the implementation that does not remove stopwords and does not stem words, list the top 10 words that have the highest conditional probability (i.e., P(w|c)) in each of the two classes considered (truth, lie). Under each class, list the words in reversed order of their conditional probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "stopped-syria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 from class true:\n",
      "['and', 'i', 'we', 'to', 'the', 'is', 'my', 'she', 'me', 'a']\n",
      "['nice', 'encourages', 'imagine', 'accomplished', 'career', 'attracts', 'attention', 'attractive', 'enviable', 'style']\n"
     ]
    }
   ],
   "source": [
    "probs = trainNaiveBayes(training_data.copy())[0]\n",
    "print('Top 10 from class true:')\n",
    "sorted_true = sorted(probs, key=lambda x:probs[x]['true'], reverse = True)\n",
    "print(sorted_true[:10])\n",
    "sorted_true = sorted(probs, key=lambda x:probs[x]['true'], reverse = False)\n",
    "print(sorted_true[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "hollywood-sigma",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 from class lie:\n",
      "['and', 'to', 'she', 'is', 'i', 'a', 'the', 'he', 'me', 'her']\n",
      "['comfort', 'joy', 'judges', 'read', 'shame', 'anyones', 'mad', 'judged', 'fight', 'agreement']\n"
     ]
    }
   ],
   "source": [
    "print('Top 10 from class lie:')\n",
    "sorted_lie = sorted(probs, key=lambda x:probs[x]['lie'], reverse = True)\n",
    "print(sorted_lie[:10])\n",
    "sorted_lie = sorted(probs, key=lambda x:probs[x]['lie'], reverse = False)\n",
    "print(sorted_lie[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
